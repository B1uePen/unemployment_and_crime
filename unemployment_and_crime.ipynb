{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jFQzOrHGIsP"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "from scipy import stats\n",
        "\n",
        "# 2. Load combined CSV file\n",
        "print(\"Upload the combined CSV file with unemployment and crime data\")\n",
        "uploaded = files.upload()\n",
        "file_name = next(iter(uploaded))\n",
        "df = pd.read_csv(BytesIO(uploaded[file_name]))\n",
        "print(f\"File '{file_name}' successfully loaded and read.\")\n",
        "\n",
        "# 3. Transform data to \"long\" format\n",
        "year_columns = [str(year) for year in range(1998, 2023)]  # Year columns\n",
        "\n",
        "# Split data into Unemployment and Crime\n",
        "df_unemployment = df[df['Index'] == 'Unemployment'].copy()\n",
        "df_crime = df[df['Index'] == 'Crime'].copy()\n",
        "\n",
        "print(f\"Found {len(df_unemployment)} unemployment records\")\n",
        "print(f\"Found {len(df_crime)} crime records\")\n",
        "\n",
        "# Transform Unemployment to \"long\" format\n",
        "df_unemployment_melted = pd.melt(\n",
        "    df_unemployment,\n",
        "    id_vars=['Country Code', 'Country name'],\n",
        "    value_vars=year_columns,\n",
        "    var_name='Year',\n",
        "    value_name='UnemploymentIndex'\n",
        ")\n",
        "\n",
        "# Transform Crime to \"long\" format\n",
        "df_crime_melted = pd.melt(\n",
        "    df_crime,\n",
        "    id_vars=['Country Code', 'Country name'],\n",
        "    value_vars=year_columns,\n",
        "    var_name='Year',\n",
        "    value_name='CrimeIndex'\n",
        ")\n",
        "\n",
        "# Merge data - only records with both indicators\n",
        "df_combined = pd.merge(\n",
        "    df_unemployment_melted,\n",
        "    df_crime_melted,\n",
        "    on=['Country Code', 'Country name', 'Year'],\n",
        "    how='inner'  # Keep only records with both indicators\n",
        ")\n",
        "\n",
        "# Convert 'Year' to numeric format\n",
        "df_combined['Year'] = df_combined['Year'].astype(int)\n",
        "\n",
        "# 4. Map country codes to regions\n",
        "region_map = {\n",
        "    'Africa': [\n",
        "        'AGO', 'BDI', 'BEN', 'BFA', 'BWA', 'CAF', 'CIV', 'CMR', 'COD', 'COG', 'COM',\n",
        "        'CPV', 'DJI', 'DZA', 'EGY', 'ERI', 'ETH', 'GAB', 'GHA', 'GIN', 'GMB', 'GNB',\n",
        "        'GNQ', 'KEN', 'LBR', 'LBY', 'LSO', 'MAR', 'MDG', 'MLI', 'MOZ', 'MRT', 'MUS',\n",
        "        'MWI', 'NAM', 'NER', 'NGA', 'RWA', 'SDN', 'SEN', 'SLE', 'SOM', 'SSD', 'STP',\n",
        "        'SWZ', 'SYC', 'TCD', 'TGO', 'TUN', 'TZA', 'UGA', 'ZAF', 'ZMB', 'ZWE'\n",
        "    ],\n",
        "    'Europe': [\n",
        "        'ALB', 'AND', 'AUT', 'BEL', 'BIH', 'BGR', 'BLR', 'CHE', 'CYP', 'CZE', 'DEU',\n",
        "        'DNK', 'ESP', 'EST', 'FIN', 'FRA', 'GBR', 'GRC', 'HRV', 'HUN', 'IRL', 'ISL',\n",
        "        'ITA', 'KOS', 'LIE', 'LTU', 'LUX', 'LVA', 'MCO', 'MDA', 'MKD', 'MLT', 'MNE',\n",
        "        'NLD', 'NOR', 'POL', 'PRT', 'ROU', 'RUS', 'SMR', 'SRB', 'SVK', 'SVN', 'SWE',\n",
        "        'UKR', 'VAT', 'XKX'\n",
        "    ],\n",
        "    'Asia': [\n",
        "        'AFG', 'ARM', 'AZE', 'BHR', 'BGD', 'BRN', 'BTN', 'CHN', 'CYP', 'GEO', 'HKG',\n",
        "        'IDN', 'IND', 'IRN', 'IRQ', 'ISR', 'JOR', 'JPN', 'KAZ', 'KGZ', 'KHM', 'KOR',\n",
        "        'KWT', 'LAO', 'LBN', 'LKA', 'MAC', 'MDV', 'MMR', 'MNG', 'MYS', 'NPL', 'OMN',\n",
        "        'PAK', 'PHL', 'PRK', 'PSE', 'QAT', 'SAU', 'SGP', 'SYR', 'THA', 'TJK', 'TKM',\n",
        "        'TLS', 'TUR', 'UZB', 'VNM', 'YEM'\n",
        "    ],\n",
        "\n",
        "      # I did not take these regions through the small number of countries in them but you can add them\n",
        "\n",
        "    # 'North America': [\n",
        "    #     'USA', 'CAN', 'BMU', 'GRL', 'CYM', 'TCA'\n",
        "    # ],\n",
        "    # 'South America': [\n",
        "    #     'ARG', 'BOL', 'BRA', 'CHL', 'COL', 'ECU', 'GUY', 'PRY', 'PER', 'SUR', 'URY', 'VEN'\n",
        "    # ],\n",
        "    # 'Oceania': [\n",
        "    #     'AUS', 'FJI', 'FSM', 'KIR', 'MHL', 'NRU', 'NZL', 'PLW', 'PNG', 'SLB', 'TON',\n",
        "    #     'TUV', 'VUT', 'WSM'\n",
        "    # ]\n",
        "}\n",
        "\n",
        "# Function to determine region\n",
        "def get_region(country_code):\n",
        "    for region, countries in region_map.items():\n",
        "        if country_code in countries:\n",
        "            return region\n",
        "    return None\n",
        "\n",
        "# Add region column\n",
        "df_combined['Region'] = df_combined['Country Code'].apply(get_region)\n",
        "\n",
        "# Function to clean numeric values\n",
        "def clean_numeric(value):\n",
        "    if pd.isna(value):\n",
        "        return np.nan\n",
        "\n",
        "    str_val = str(value).strip()\n",
        "\n",
        "    # Check for invalid formats (like \"10.05.25\")\n",
        "    if '.' in str_val and len(str_val.split('.')) > 2:\n",
        "        return np.nan\n",
        "\n",
        "    try:\n",
        "        return float(str_val)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "df_combined['UnemploymentIndex'] = df_combined['UnemploymentIndex'].apply(clean_numeric)\n",
        "df_combined['CrimeIndex'] = df_combined['CrimeIndex'].apply(clean_numeric)\n",
        "\n",
        "# Remove rows with NaN values or without assigned region\n",
        "df_clean = df_combined.dropna(subset=['UnemploymentIndex', 'CrimeIndex', 'Region'])\n",
        "\n",
        "print(f\"After cleaning, {len(df_clean)} records remain\")\n",
        "\n",
        "# 5. CUSTOM MATHEMATICAL FUNCTIONS\n",
        "\n",
        "import math\n",
        "\n",
        "# 1. Manual descriptive statistics\n",
        "def describe_manual(data):\n",
        "    \"\"\"Calculate mean, median, variance (population), std, min, max manually.\"\"\"\n",
        "    n = len(data)\n",
        "    if n == 0:\n",
        "        return None\n",
        "    sorted_data = sorted(data)  # Sort for median, min, max\n",
        "    mean_val = sum(data) / n\n",
        "    # median\n",
        "    if n % 2 == 1:\n",
        "        median_val = sorted_data[n//2]\n",
        "    else:\n",
        "        median_val = (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2\n",
        "    # variance and std (for population)\n",
        "    var_val = sum((x - mean_val)**2 for x in data) / n\n",
        "    std_val = math.sqrt(var_val)\n",
        "    min_val = sorted_data[0]\n",
        "    max_val = sorted_data[-1]\n",
        "    return mean_val, median_val, var_val, std_val, min_val, max_val\n",
        "\n",
        "# 2. IQR analysis manually (for outlier detection)\n",
        "def iqr_outliers(data):\n",
        "    \"\"\"Calculate Q1, Q3, IQR and find outliers using 1.5*IQR rule.\"\"\"\n",
        "    if not data:\n",
        "        return [], None, None, None\n",
        "    sorted_data = sorted(data)\n",
        "    n = len(sorted_data)\n",
        "    # Find median and split into two halves\n",
        "    if n % 2 == 1:\n",
        "        lower = sorted_data[:n//2]\n",
        "        upper = sorted_data[n//2+1:]\n",
        "    else:\n",
        "        lower = sorted_data[:n//2]\n",
        "        upper = sorted_data[n//2:]\n",
        "    # Function to calculate median of a list\n",
        "    def median(lst):\n",
        "        m = len(lst)\n",
        "        return (lst[m//2] if m%2 else (lst[m//2 - 1] + lst[m//2]) / 2) if m>0 else None\n",
        "    Q1 = median(lower)\n",
        "    Q3 = median(upper)\n",
        "    if Q1 is None or Q3 is None:\n",
        "        return [], Q1, Q3, None\n",
        "    IQR = Q3 - Q1\n",
        "    lower_fence = Q1 - 1.5 * IQR\n",
        "    upper_fence = Q3 + 1.5 * IQR\n",
        "    outliers = [x for x in data if x < lower_fence or x > upper_fence]\n",
        "    return outliers, Q1, Q3, IQR\n",
        "\n",
        "# 3. Spearman correlation with p-value\n",
        "def spearman_correlation_with_pvalue(x, y):\n",
        "    \"\"\"Spearman correlation with p-value\"\"\"\n",
        "    n = len(x)\n",
        "    if n < 3:\n",
        "        return None, None\n",
        "\n",
        "    # Ranking function\n",
        "    def rankdata(arr):\n",
        "        # Create list of pairs (value, original index)\n",
        "        indexed_values = [(val, idx) for idx, val in enumerate(arr)]\n",
        "        # Sort by value\n",
        "        indexed_values.sort(key=lambda x: x[0])\n",
        "\n",
        "        ranks = [0] * len(arr)\n",
        "        i = 0\n",
        "\n",
        "        while i < len(indexed_values):\n",
        "            # Find all elements with the same value\n",
        "            current_value = indexed_values[i][0]\n",
        "            j = i\n",
        "            while j < len(indexed_values) and indexed_values[j][0] == current_value:\n",
        "                j += 1\n",
        "\n",
        "            # Calculate average rank for tied values\n",
        "            avg_rank = (2 * i + j - i + 1) / 2.0  # Average of ranks\n",
        "\n",
        "            # Assign average rank to all tied values\n",
        "            for k in range(i, j):\n",
        "                original_index = indexed_values[k][1]\n",
        "                ranks[original_index] = avg_rank\n",
        "\n",
        "            i = j\n",
        "\n",
        "        return ranks\n",
        "\n",
        "    # Calculate ranks\n",
        "    rank_x = rankdata(x)\n",
        "    rank_y = rankdata(y)\n",
        "\n",
        "    # Calculate Pearson correlation for ranks\n",
        "    def pearson_for_ranks(rx, ry):\n",
        "        n = len(rx)\n",
        "        mean_rx = sum(rx) / n\n",
        "        mean_ry = sum(ry) / n\n",
        "\n",
        "        numerator = sum((rx[i] - mean_rx) * (ry[i] - mean_ry) for i in range(n))\n",
        "        sum_sq_x = sum((rx[i] - mean_rx)**2 for i in range(n))\n",
        "        sum_sq_y = sum((ry[i] - mean_ry)**2 for i in range(n))\n",
        "\n",
        "        if sum_sq_x == 0 or sum_sq_y == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return numerator / math.sqrt(sum_sq_x * sum_sq_y)\n",
        "\n",
        "    rho = pearson_for_ranks(rank_x, rank_y)\n",
        "\n",
        "    # Calculate p-value for Spearman correlation\n",
        "    if abs(rho) >= 0.999:\n",
        "        return rho, 0.0\n",
        "\n",
        "    df = n - 2\n",
        "    if df <= 0:\n",
        "        return rho, 1.0\n",
        "\n",
        "    # t-statistic for Spearman correlation\n",
        "    try:\n",
        "        t_stat = rho * math.sqrt(df / (1 - rho**2))\n",
        "    except (ZeroDivisionError, ValueError):\n",
        "        return rho, 0.0 if abs(rho) > 0.8 else 1.0\n",
        "\n",
        "    # Calculate p-value\n",
        "    def spearman_pvalue(t, df):\n",
        "        \"\"\"P-value for Spearman correlation\"\"\"\n",
        "        abs_t = abs(t)\n",
        "\n",
        "        if df >= 30:\n",
        "            # For large samples, use normal approximation\n",
        "            if abs_t > 3.3:\n",
        "                return 0.001\n",
        "            elif abs_t > 2.6:\n",
        "                return 0.01\n",
        "            elif abs_t > 2.0:\n",
        "                return 0.05\n",
        "            elif abs_t > 1.6:\n",
        "                return 0.1\n",
        "            elif abs_t > 1.0:\n",
        "                return 0.3\n",
        "            else:\n",
        "                return 0.6\n",
        "        else:\n",
        "            # For small samples, more conservative estimates\n",
        "            if abs_t > 3.0:\n",
        "                return 0.01\n",
        "            elif abs_t > 2.5:\n",
        "                return 0.05\n",
        "            elif abs_t > 2.0:\n",
        "                return 0.1\n",
        "            elif abs_t > 1.5:\n",
        "                return 0.2\n",
        "            elif abs_t > 1.0:\n",
        "                return 0.4\n",
        "            else:\n",
        "                return 0.7\n",
        "\n",
        "    p_val = spearman_pvalue(t_stat, df)\n",
        "    return rho, min(p_val, 1.0)\n",
        "\n",
        "# 4. Linear regression manually\n",
        "def linear_regression(x, y):\n",
        "    \"\"\"Calculate α (intercept), β (slope) and R² without sklearn/statsmodels.\"\"\"\n",
        "    n = len(x)\n",
        "    mean_x = sum(x)/n\n",
        "    mean_y = sum(y)/n\n",
        "    cov = sum((xi - mean_x)*(yi - mean_y) for xi, yi in zip(x, y))\n",
        "    var_x = sum((xi - mean_x)**2 for xi in x)\n",
        "    if var_x == 0:\n",
        "        return None\n",
        "    beta = cov / var_x\n",
        "    alpha = mean_y - beta * mean_x\n",
        "\n",
        "    ss_tot = sum((yi - mean_y)**2 for yi in y)\n",
        "    ss_reg = sum((beta*xi + alpha - mean_y)**2 for xi in x)\n",
        "    r2 = ss_reg / ss_tot if ss_tot != 0 else None\n",
        "    return alpha, beta, r2\n",
        "\n",
        "# Helper functions\n",
        "def calculate_mean(values):\n",
        "    \"\"\"Arithmetic mean\"\"\"\n",
        "    result = describe_manual(values)\n",
        "    return result[0] if result else 0\n",
        "\n",
        "def calculate_median(values):\n",
        "    \"\"\"Median\"\"\"\n",
        "    result = describe_manual(values)\n",
        "    return result[1] if result else 0\n",
        "\n",
        "def calculate_variance(values, sample=True):\n",
        "    \"\"\"Variance\"\"\"\n",
        "    result = describe_manual(values)\n",
        "    if result is None:\n",
        "        return 0\n",
        "    if sample and len(values) > 1:\n",
        "        # Convert from population to sample variance\n",
        "        return result[2] * len(values) / (len(values) - 1)\n",
        "    return result[2]\n",
        "\n",
        "def calculate_std(values, sample=True):\n",
        "    \"\"\"Standard deviation\"\"\"\n",
        "    return math.sqrt(calculate_variance(values, sample))\n",
        "\n",
        "def detect_outliers_iqr(values):\n",
        "    \"\"\"Detect anomalies using IQR method\"\"\"\n",
        "    if len(values) <= 2:\n",
        "        return [], values\n",
        "    outliers, q1, q3, iqr = iqr_outliers(values)\n",
        "    normal_values = [x for x in values if x not in outliers]\n",
        "    return outliers, normal_values\n",
        "\n",
        "def replace_outliers_with_median(values):\n",
        "    \"\"\"Replace anomalies with median\"\"\"\n",
        "    if len(values) <= 2:\n",
        "        return values\n",
        "\n",
        "    outliers, normal_values = detect_outliers_iqr(values)\n",
        "\n",
        "    if len(normal_values) == 0:\n",
        "        return values\n",
        "\n",
        "    median_val = calculate_median(normal_values)\n",
        "\n",
        "    result = []\n",
        "    for val in values:\n",
        "        if val in outliers:\n",
        "            result.append(median_val)\n",
        "        else:\n",
        "            result.append(val)\n",
        "\n",
        "    return result\n",
        "\n",
        "def calculate_skewness(values):\n",
        "    \"\"\"Coefficient of asymmetry (skewness)\"\"\"\n",
        "    if len(values) < 3:\n",
        "        return 0\n",
        "\n",
        "    n = len(values)\n",
        "    mean_val = calculate_mean(values)\n",
        "    std_val = calculate_std(values, sample=False)\n",
        "\n",
        "    if std_val == 0:\n",
        "        return 0\n",
        "\n",
        "    skew_sum = sum(((x - mean_val) / std_val) ** 3 for x in values)\n",
        "    return skew_sum / n\n",
        "\n",
        "def calculate_kurtosis(values):\n",
        "    \"\"\"Coefficient of excess kurtosis\"\"\"\n",
        "    if len(values) < 4:\n",
        "        return 0\n",
        "\n",
        "    n = len(values)\n",
        "    mean_val = calculate_mean(values)\n",
        "    std_val = calculate_std(values, sample=False)\n",
        "\n",
        "    if std_val == 0:\n",
        "        return 0\n",
        "\n",
        "    kurt_sum = sum(((x - mean_val) / std_val) ** 4 for x in values)\n",
        "    return (kurt_sum / n) - 3  # Excess kurtosis relative to normal distribution\n",
        "\n",
        "def test_normality(values):\n",
        "    \"\"\"Simple test for distribution normality\"\"\"\n",
        "    if len(values) < 8:\n",
        "        return \"Insufficient data\", True  # Assume normality\n",
        "\n",
        "    skew = calculate_skewness(values)\n",
        "    kurt = calculate_kurtosis(values)\n",
        "\n",
        "    # Criteria for normality (approximate)\n",
        "    is_normal = abs(skew) < 2 and abs(kurt) < 2\n",
        "\n",
        "    interpretation = []\n",
        "    if abs(skew) >= 2:\n",
        "        interpretation.append(f\"High asymmetry ({skew:.2f})\")\n",
        "    if abs(kurt) >= 2:\n",
        "        interpretation.append(f\"High kurtosis ({kurt:.2f})\")\n",
        "\n",
        "    if is_normal:\n",
        "        return \"Approximately normal\", True\n",
        "    else:\n",
        "        return f\"Not normal: {', '.join(interpretation)}\", False\n",
        "\n",
        "def calculate_spearman_correlation(x_values, y_values):\n",
        "    \"\"\"Spearman correlation coefficient with p-value\"\"\"\n",
        "    if len(x_values) != len(y_values) or len(x_values) < 2:\n",
        "        return 0, None\n",
        "\n",
        "    result = spearman_correlation_with_pvalue(x_values, y_values)\n",
        "    if result[0] is None:\n",
        "        return 0, None\n",
        "    return result[0], result[1]\n",
        "\n",
        "# 6. ANALYSIS BY REGIONS AND YEARS (WITH USER PROMPT)\n",
        "print(\"\\n=== ANALYSIS BY REGIONS AND YEARS ===\")\n",
        "\n",
        "# Ask user if they want to see detailed statistics\n",
        "show_details = input(\"\\nDo you want to display detailed yearly statistics for each region? (yes/no): \").strip().lower()\n",
        "\n",
        "regions = df_clean['Region'].unique()\n",
        "years = sorted(df_clean['Year'].unique())\n",
        "\n",
        "# Dictionaries to store results\n",
        "regional_stats = {}\n",
        "correlation_results = {}\n",
        "regression_results = {}\n",
        "normality_results = {}\n",
        "\n",
        "for region in regions:\n",
        "    if show_details == 'yes':\n",
        "        print(f\"\\n=== REGION: {region} ===\")\n",
        "\n",
        "    regional_stats[region] = {}\n",
        "    correlation_results[region] = {}\n",
        "    regression_results[region] = {}\n",
        "    normality_results[region] = {}\n",
        "\n",
        "    region_data = df_clean[df_clean['Region'] == region]\n",
        "\n",
        "    for year in years:\n",
        "        year_data = region_data[region_data['Year'] == year]\n",
        "\n",
        "        # Check if there's data for this year\n",
        "        if len(year_data) == 0:\n",
        "            continue\n",
        "\n",
        "        if show_details == 'yes':\n",
        "            print(f\"\\n--- Year {year} ---\")\n",
        "            print(f\"Number of countries with both coefficients: {len(year_data)}\")\n",
        "\n",
        "        # Get values\n",
        "        unemp_vals = year_data['UnemploymentIndex'].values.tolist()\n",
        "        crime_vals = year_data['CrimeIndex'].values.tolist()\n",
        "\n",
        "        # Test distributions\n",
        "        unemp_normality, unemp_is_normal = test_normality(unemp_vals)\n",
        "        crime_normality, crime_is_normal = test_normality(crime_vals)\n",
        "\n",
        "        if show_details == 'yes':\n",
        "            print(f\"Unemployment distribution: {unemp_normality}\")\n",
        "            print(f\"Crime distribution: {crime_normality}\")\n",
        "\n",
        "        normality_results[region][year] = {\n",
        "            'unemployment_normal': unemp_is_normal,\n",
        "            'crime_normal': crime_is_normal\n",
        "        }\n",
        "\n",
        "        # Detect and handle anomalies\n",
        "        if show_details == 'yes':\n",
        "            print(\"Anomaly analysis:\")\n",
        "        unemp_outliers, unemp_normal = detect_outliers_iqr(unemp_vals)\n",
        "        crime_outliers, crime_normal = detect_outliers_iqr(crime_vals)\n",
        "\n",
        "        if show_details == 'yes':\n",
        "            print(f\"  Unemployment: {len(unemp_outliers)} anomalies out of {len(unemp_vals)} values\")\n",
        "            print(f\"  Crime: {len(crime_outliers)} anomalies out of {len(crime_vals)} values\")\n",
        "\n",
        "        # Replace anomalies with median\n",
        "        unemp_cleaned = replace_outliers_with_median(unemp_vals)\n",
        "        crime_cleaned = replace_outliers_with_median(crime_vals)\n",
        "\n",
        "        # Descriptive statistics\n",
        "        unemp_stats = describe_manual(unemp_cleaned)\n",
        "        crime_stats = describe_manual(crime_cleaned)\n",
        "\n",
        "        stats = {\n",
        "            'unemployment': {\n",
        "                'mean': unemp_stats[0] if unemp_stats else 0,\n",
        "                'median': unemp_stats[1] if unemp_stats else 0,\n",
        "                'variance': calculate_variance(unemp_cleaned),\n",
        "                'std': calculate_std(unemp_cleaned),\n",
        "                'min': unemp_stats[4] if unemp_stats else 0,\n",
        "                'max': unemp_stats[5] if unemp_stats else 0,\n",
        "                'skewness': calculate_skewness(unemp_cleaned),\n",
        "                'kurtosis': calculate_kurtosis(unemp_cleaned)\n",
        "            },\n",
        "            'crime': {\n",
        "                'mean': crime_stats[0] if crime_stats else 0,\n",
        "                'median': crime_stats[1] if crime_stats else 0,\n",
        "                'variance': calculate_variance(crime_cleaned),\n",
        "                'std': calculate_std(crime_cleaned),\n",
        "                'min': crime_stats[4] if crime_stats else 0,\n",
        "                'max': crime_stats[5] if crime_stats else 0,\n",
        "                'skewness': calculate_skewness(crime_cleaned),\n",
        "                'kurtosis': calculate_kurtosis(crime_cleaned)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        regional_stats[region][year] = stats\n",
        "\n",
        "        if show_details == 'yes':\n",
        "            print(\"Descriptive statistics (after handling anomalies):\")\n",
        "            print(f\"  Unemployment: μ={stats['unemployment']['mean']:.2f}, Med={stats['unemployment']['median']:.2f}, σ={stats['unemployment']['std']:.2f}\")\n",
        "            print(f\"  Crime: μ={stats['crime']['mean']:.2f}, Med={stats['crime']['median']:.2f}, σ={stats['crime']['std']:.2f}\")\n",
        "\n",
        "        # Calculate Spearman correlation\n",
        "        correlation, p_value_spearman = calculate_spearman_correlation(unemp_cleaned, crime_cleaned)\n",
        "\n",
        "        correlation_results[region][year] = {\n",
        "            'correlation': correlation,\n",
        "            'p_value': p_value_spearman\n",
        "        }\n",
        "\n",
        "        if show_details == 'yes':\n",
        "            print(f\"Spearman Correlation: r = {correlation:.3f}\")\n",
        "            if p_value_spearman is not None:\n",
        "                significance = \"***\" if p_value_spearman < 0.001 else \"**\" if p_value_spearman < 0.01 else \"*\" if p_value_spearman < 0.05 else \"\"\n",
        "                print(f\"p-value: {p_value_spearman:.6f} {significance}\")\n",
        "\n",
        "        # Linear regression\n",
        "        if len(unemp_cleaned) >= 2:\n",
        "            reg_result = linear_regression(unemp_cleaned, crime_cleaned)\n",
        "            if reg_result is not None:\n",
        "                a, b, r_squared = reg_result\n",
        "\n",
        "                regression_results[region][year] = {\n",
        "                    'intercept': a,\n",
        "                    'slope': b,\n",
        "                    'r_squared': r_squared if r_squared is not None else 0\n",
        "                }\n",
        "\n",
        "                if show_details == 'yes':\n",
        "                    print(f\"Linear regression: Crime = {a:.3f} + {b:.3f} * Unemployment\")\n",
        "                    print(f\"Coefficient of determination: R² = {r_squared:.3f}\" if r_squared is not None else \"R² = N/A\")\n",
        "\n",
        "if show_details != 'yes':\n",
        "    print(\"\\nDetailed statistics skipped. Proceeding to visualizations...\\n\")\n",
        "\n",
        "# 7. INDEX DISTRIBUTION HISTOGRAMS FOR EACH REGION\n",
        "print(\"\\n=== INDEX DISTRIBUTION HISTOGRAMS ===\")\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "for region in regions:\n",
        "    region_data = df_clean[df_clean['Region'] == region]\n",
        "\n",
        "    if len(region_data) == 0:\n",
        "        continue\n",
        "\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle(f'Index Distributions: {region}', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Unemployment histogram\n",
        "    unemp_all = region_data['UnemploymentIndex'].values\n",
        "    ax1.hist(unemp_all, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
        "    ax1.set_title('Unemployment Index Distribution')\n",
        "    ax1.set_xlabel('Unemployment Index')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Statistics for unemployment\n",
        "    unemp_stats = describe_manual(unemp_all.tolist())\n",
        "    if unemp_stats:\n",
        "        unemp_mean, unemp_median = unemp_stats[0], unemp_stats[1]\n",
        "        ax1.axvline(unemp_mean, color='red', linestyle='--', label=f'Mean: {unemp_mean:.2f}')\n",
        "        ax1.axvline(unemp_median, color='green', linestyle='--', label=f'Median: {unemp_median:.2f}')\n",
        "        ax1.legend()\n",
        "\n",
        "    # Crime histogram\n",
        "    crime_all = region_data['CrimeIndex'].values\n",
        "    ax2.hist(crime_all, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
        "    ax2.set_title('Crime Index Distribution')\n",
        "    ax2.set_xlabel('Crime Index')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Statistics for crime\n",
        "    crime_stats = describe_manual(crime_all.tolist())\n",
        "    if crime_stats:\n",
        "        crime_mean, crime_median = crime_stats[0], crime_stats[1]\n",
        "        ax2.axvline(crime_mean, color='red', linestyle='--', label=f'Mean: {crime_mean:.2f}')\n",
        "        ax2.axvline(crime_median, color='green', linestyle='--', label=f'Median: {crime_median:.2f}')\n",
        "        ax2.legend()\n",
        "\n",
        "    # Q-Q plot for unemployment\n",
        "    unemp_sorted = sorted(unemp_all)\n",
        "    n = len(unemp_sorted)\n",
        "    theoretical_quantiles = [(i - 0.5) / n for i in range(1, n + 1)]\n",
        "    norm_quantiles = [math.sqrt(2) * (2 * q - 1) for q in theoretical_quantiles]\n",
        "\n",
        "    ax3.scatter(norm_quantiles, unemp_sorted, alpha=0.6)\n",
        "    ax3.plot([min(norm_quantiles), max(norm_quantiles)],\n",
        "             [min(unemp_sorted), max(unemp_sorted)], 'r--')\n",
        "    ax3.set_title('Q-Q plot: Unemployment vs Normal Distribution')\n",
        "    ax3.set_xlabel('Theoretical Quantiles')\n",
        "    ax3.set_ylabel('Sample Quantiles')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Q-Q plot for crime\n",
        "    crime_sorted = sorted(crime_all)\n",
        "    ax4.scatter(norm_quantiles[:len(crime_sorted)], crime_sorted, alpha=0.6, color='red')\n",
        "    ax4.plot([min(norm_quantiles[:len(crime_sorted)]), max(norm_quantiles[:len(crime_sorted)])],\n",
        "             [min(crime_sorted), max(crime_sorted)], 'r--')\n",
        "    ax4.set_title('Q-Q plot: Crime vs Normal Distribution')\n",
        "    ax4.set_xlabel('Theoretical Quantiles')\n",
        "    ax4.set_ylabel('Sample Quantiles')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Output distribution statistics\n",
        "    print(f\"\\n{region} - Distribution Statistics:\")\n",
        "    unemp_skew = calculate_skewness(unemp_all.tolist())\n",
        "    unemp_kurt = calculate_kurtosis(unemp_all.tolist())\n",
        "    crime_skew = calculate_skewness(crime_all.tolist())\n",
        "    crime_kurt = calculate_kurtosis(crime_all.tolist())\n",
        "\n",
        "    print(f\"Unemployment: skewness={unemp_skew:.3f}, kurtosis={unemp_kurt:.3f}\")\n",
        "    print(f\"Crime: skewness={crime_skew:.3f}, kurtosis={crime_kurt:.3f}\")\n",
        "\n",
        "# 8. CORRELATION HEATMAP\n",
        "print(\"\\n=== CORRELATION HEATMAP ===\")\n",
        "\n",
        "# Create correlation matrix by regions and years\n",
        "correlation_matrix_data = []\n",
        "regions_list = sorted(regions)\n",
        "years_list = sorted(years)\n",
        "\n",
        "for region in regions_list:\n",
        "    row = []\n",
        "    for year in years_list:\n",
        "        if region in correlation_results and year in correlation_results[region]:\n",
        "            corr_val = correlation_results[region][year]['correlation']\n",
        "            row.append(corr_val)\n",
        "        else:\n",
        "            row.append(np.nan)\n",
        "    correlation_matrix_data.append(row)\n",
        "\n",
        "correlation_matrix = np.array(correlation_matrix_data)\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(20, 8))\n",
        "mask = np.isnan(correlation_matrix)\n",
        "sns.heatmap(correlation_matrix,\n",
        "            xticklabels=years_list,\n",
        "            yticklabels=regions_list,\n",
        "            annot=True,\n",
        "            fmt='.3f',\n",
        "            cmap='RdBu_r',\n",
        "            center=0,\n",
        "            mask=mask,\n",
        "            cbar_kws={'label': 'Spearman Correlation Coefficient'},\n",
        "            square=False)\n",
        "\n",
        "plt.title('Heatmap of Spearman Correlations between Unemployment and Crime\\n(by Regions and Years)',\n",
        "          fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Region', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional heatmap - average correlations by regions\n",
        "print(\"\\n=== AVERAGE CORRELATIONS BY REGIONS HEATMAP ===\")\n",
        "\n",
        "avg_correlations = []\n",
        "region_names = []\n",
        "\n",
        "for region in regions_list:\n",
        "    if region in correlation_results and len(correlation_results[region]) > 0:\n",
        "        correlations = [data['correlation'] for data in correlation_results[region].values()\n",
        "                       if data['correlation'] is not None and not np.isnan(data['correlation'])]\n",
        "        if correlations:\n",
        "            avg_corr = calculate_mean(correlations)\n",
        "            avg_correlations.append([avg_corr])\n",
        "            region_names.append(f\"{region}\\n(n={len(correlations)})\")\n",
        "\n",
        "if avg_correlations:\n",
        "    plt.figure(figsize=(3, 8))\n",
        "    sns.heatmap(avg_correlations,\n",
        "                yticklabels=region_names,\n",
        "                annot=True,\n",
        "                fmt='.3f',\n",
        "                cmap='RdBu_r',\n",
        "                center=0,\n",
        "                cbar_kws={'label': 'Average Spearman Correlation'},\n",
        "                square=False)\n",
        "\n",
        "    plt.title('Average Spearman Correlations by Region', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('Region (number of years)', fontsize=12)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 9. RESULTS VISUALIZATION\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# 9.1 Correlation graphs by years for each region\n",
        "for region in regions:\n",
        "    if region not in correlation_results or len(correlation_results[region]) == 0:\n",
        "        continue\n",
        "\n",
        "    years_with_data = sorted(correlation_results[region].keys())\n",
        "    correlations = [correlation_results[region][year]['correlation'] for year in years_with_data]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(years_with_data, correlations, marker='o', linewidth=2, markersize=8)\n",
        "    plt.title(f'Spearman Correlation by Years: {region}')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Spearman Correlation Coefficient')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim(-1, 1)\n",
        "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Add horizontal lines for interpretation\n",
        "    plt.axhline(y=0.3, color='green', linestyle=':', alpha=0.7, label='Weak Positive')\n",
        "    plt.axhline(y=-0.3, color='red', linestyle=':', alpha=0.7, label='Weak Negative')\n",
        "    plt.axhline(y=0.7, color='darkgreen', linestyle=':', alpha=0.7, label='Strong Positive')\n",
        "    plt.axhline(y=-0.7, color='darkred', linestyle=':', alpha=0.7, label='Strong Negative')\n",
        "\n",
        "    # Add number of countries for each point\n",
        "    for i, (year, corr) in enumerate(zip(years_with_data, correlations)):\n",
        "        region_year_data = df_clean[(df_clean['Region'] == region) & (df_clean['Year'] == year)]\n",
        "        n_countries = len(region_year_data)\n",
        "        plt.annotate(f'n={n_countries}',\n",
        "                    (year, corr),\n",
        "                    textcoords=\"offset points\",\n",
        "                    xytext=(0,10),\n",
        "                    ha='center',\n",
        "                    fontsize=8,\n",
        "                    alpha=0.7)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 9.2 Scatter plots with regression for the latest year with data\n",
        "print(\"\\n=== SCATTER PLOTS WITH REGRESSION (Latest Year with Data) ===\")\n",
        "\n",
        "for region in regions:\n",
        "    region_data = df_clean[df_clean['Region'] == region]\n",
        "\n",
        "    if len(region_data) == 0:\n",
        "        continue\n",
        "\n",
        "    # Find latest year with data\n",
        "    latest_year = None\n",
        "    for year in sorted(region_data['Year'].unique(), reverse=True):\n",
        "        year_data = region_data[region_data['Year'] == year]\n",
        "        if len(year_data) >= 1:\n",
        "            latest_year = year\n",
        "            break\n",
        "\n",
        "    if latest_year is None:\n",
        "        continue\n",
        "\n",
        "    year_data = region_data[region_data['Year'] == latest_year]\n",
        "\n",
        "    # Handle anomalies\n",
        "    unemp_vals = year_data['UnemploymentIndex'].values.tolist()\n",
        "    crime_vals = year_data['CrimeIndex'].values.tolist()\n",
        "    unemp_cleaned = replace_outliers_with_median(unemp_vals)\n",
        "    crime_cleaned = replace_outliers_with_median(crime_vals)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Different markers depending on number of countries\n",
        "    if len(year_data) == 1:\n",
        "        plt.scatter(unemp_cleaned, crime_cleaned, alpha=0.8, s=200, c='red',\n",
        "                   marker='*', label=f'Data from {latest_year} (1 country)')\n",
        "    else:\n",
        "        plt.scatter(unemp_cleaned, crime_cleaned, alpha=0.7, s=80,\n",
        "                   label=f'Data from {latest_year} ({len(year_data)} countries)')\n",
        "\n",
        "    # Add regression line only if more than 1 point\n",
        "    if len(year_data) > 1 and latest_year in regression_results.get(region, {}):\n",
        "        reg_params = regression_results[region][latest_year]\n",
        "        a, b = reg_params['intercept'], reg_params['slope']\n",
        "        r_squared = reg_params['r_squared']\n",
        "\n",
        "        x_min, x_max = min(unemp_cleaned), max(unemp_cleaned)\n",
        "        x_line = [x_min, x_max]\n",
        "        y_line = [a + b * x for x in x_line]\n",
        "\n",
        "        plt.plot(x_line, y_line, color='red', linewidth=2,\n",
        "                label=f'Regression: y = {a:.2f} + {b:.2f}x\\nR² = {r_squared:.3f}')\n",
        "\n",
        "    # Add country labels\n",
        "    for i, row in year_data.iterrows():\n",
        "        plt.annotate(row['Country Code'],\n",
        "                    (unemp_cleaned[year_data.index.get_loc(i)],\n",
        "                     crime_cleaned[year_data.index.get_loc(i)]),\n",
        "                    xytext=(5, 5), textcoords='offset points',\n",
        "                    fontsize=8, alpha=0.7)\n",
        "\n",
        "    plt.title(f'{region}: Unemployment vs Crime ({latest_year})')\n",
        "    plt.xlabel('Unemployment Index')\n",
        "    plt.ylabel('Crime Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 10. INDEX GRAPHS BY YEARS FOR EACH REGION\n",
        "print(\"\\n=== INDEX GRAPHS BY YEARS FOR EACH REGION ===\")\n",
        "\n",
        "for region in regions:\n",
        "    region_data = df_clean[df_clean['Region'] == region]\n",
        "\n",
        "    if len(region_data) == 0:\n",
        "        continue\n",
        "\n",
        "    # Group data by years\n",
        "    yearly_stats = {}\n",
        "    for year in sorted(region_data['Year'].unique()):\n",
        "        year_data = region_data[region_data['Year'] == year]\n",
        "        if len(year_data) > 0:\n",
        "            unemp_vals = year_data['UnemploymentIndex'].values\n",
        "            crime_vals = year_data['CrimeIndex'].values\n",
        "\n",
        "            yearly_stats[year] = {\n",
        "                'unemployment_mean': calculate_mean(unemp_vals.tolist()),\n",
        "                'unemployment_median': calculate_median(unemp_vals.tolist()),\n",
        "                'crime_mean': calculate_mean(crime_vals.tolist()),\n",
        "                'crime_median': calculate_median(crime_vals.tolist()),\n",
        "                'count': len(year_data)\n",
        "            }\n",
        "\n",
        "    if len(yearly_stats) == 0:\n",
        "        continue\n",
        "\n",
        "    years_list = sorted(yearly_stats.keys())\n",
        "    unemp_means = [yearly_stats[year]['unemployment_mean'] for year in years_list]\n",
        "    unemp_medians = [yearly_stats[year]['unemployment_median'] for year in years_list]\n",
        "    crime_means = [yearly_stats[year]['crime_mean'] for year in years_list]\n",
        "    crime_medians = [yearly_stats[year]['crime_median'] for year in years_list]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Create two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "    # Unemployment graph\n",
        "    ax1.plot(years_list, unemp_means, marker='o', linewidth=2, label='Mean', color='blue')\n",
        "    ax1.plot(years_list, unemp_medians, marker='s', linewidth=2, label='Median',\n",
        "             color='lightblue', linestyle='--')\n",
        "    ax1.set_title(f'{region}: Unemployment Index by Years')\n",
        "    ax1.set_ylabel('Unemployment Index')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add number of countries for each point\n",
        "    for i, year in enumerate(years_list):\n",
        "        count = yearly_stats[year]['count']\n",
        "        ax1.annotate(f'n={count}', (year, unemp_means[i]),\n",
        "                    textcoords=\"offset points\", xytext=(0,10),\n",
        "                    ha='center', fontsize=8, alpha=0.7)\n",
        "\n",
        "    # Crime graph\n",
        "    ax2.plot(years_list, crime_means, marker='o', linewidth=2, label='Mean', color='red')\n",
        "    ax2.plot(years_list, crime_medians, marker='s', linewidth=2, label='Median',\n",
        "             color='pink', linestyle='--')\n",
        "    ax2.set_title(f'{region}: Crime Index by Years')\n",
        "    ax2.set_xlabel('Year')\n",
        "    ax2.set_ylabel('Crime Index')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add number of countries for each point\n",
        "    for i, year in enumerate(years_list):\n",
        "        count = yearly_stats[year]['count']\n",
        "        ax2.annotate(f'n={count}', (year, crime_means[i]),\n",
        "                    textcoords=\"offset points\", xytext=(0,10),\n",
        "                    ha='center', fontsize=8, alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n{region} - Statistics by Years:\")\n",
        "    print(\"Year\\tUnemp(mean)\\tUnemp(med)\\tCrime(mean)\\tCrime(med)\\tCountries\")\n",
        "    for year in years_list:\n",
        "        stats = yearly_stats[year]\n",
        "        print(f\"{year}\\t{stats['unemployment_mean']:.2f}\\t\\t{stats['unemployment_median']:.2f}\\t\\t\"\n",
        "              f\"{stats['crime_mean']:.2f}\\t\\t{stats['crime_median']:.2f}\\t\\t{stats['count']}\")\n",
        "\n",
        "# 11. COMBINED GRAPHS OF UNEMPLOYMENT AND CRIME BY REGIONS\n",
        "print(\"\\n=== COMBINED GRAPHS OF UNEMPLOYMENT AND CRIME BY REGIONS ===\")\n",
        "\n",
        "# Collect all data by regions for overall analysis\n",
        "overall_stats = {}\n",
        "\n",
        "for region in regions:\n",
        "    region_data = df_clean[df_clean['Region'] == region]\n",
        "\n",
        "    if len(region_data) == 0:\n",
        "        continue\n",
        "\n",
        "    # Group data by years\n",
        "    yearly_stats = {}\n",
        "    for year in sorted(region_data['Year'].unique()):\n",
        "        year_data = region_data[region_data['Year'] == year]\n",
        "        if len(year_data) > 0:\n",
        "            unemp_vals = year_data['UnemploymentIndex'].values\n",
        "            crime_vals = year_data['CrimeIndex'].values\n",
        "\n",
        "            yearly_stats[year] = {\n",
        "                'unemployment_mean': calculate_mean(unemp_vals.tolist()),\n",
        "                'crime_mean': calculate_mean(crime_vals.tolist()),\n",
        "                'count': len(year_data)\n",
        "            }\n",
        "\n",
        "    if len(yearly_stats) == 0:\n",
        "        continue\n",
        "\n",
        "    years_list = sorted(yearly_stats.keys())\n",
        "    unemp_means = [yearly_stats[year]['unemployment_mean'] for year in years_list]\n",
        "    crime_means = [yearly_stats[year]['crime_mean'] for year in years_list]\n",
        "\n",
        "    # Create combined graph\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "    # Determine common limits for both Y axes\n",
        "    all_values = unemp_means + crime_means\n",
        "    global_min = min(all_values)\n",
        "    global_max = max(all_values)\n",
        "    padding = (global_max - global_min) * 0.1\n",
        "    y_min = global_min - padding\n",
        "    y_max = global_max + padding\n",
        "\n",
        "    # Unemployment graph (left Y axis)\n",
        "    color1 = 'tab:blue'\n",
        "    ax1.set_xlabel('Year', fontsize=12)\n",
        "    ax1.set_ylabel('Unemployment Index', color=color1, fontsize=12)\n",
        "    line1 = ax1.plot(years_list, unemp_means, marker='o', linewidth=3,\n",
        "                     label='Unemployment', color=color1, markersize=8)\n",
        "    ax1.tick_params(axis='y', labelcolor=color1)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(y_min, y_max)\n",
        "\n",
        "    # Create second Y axis for crime\n",
        "    ax2 = ax1.twinx()\n",
        "    color2 = 'tab:red'\n",
        "    ax2.set_ylabel('Crime Index', color=color2, fontsize=12)\n",
        "    line2 = ax2.plot(years_list, crime_means, marker='s', linewidth=3,\n",
        "                     label='Crime', color=color2, markersize=8, linestyle='--')\n",
        "    ax2.tick_params(axis='y', labelcolor=color2)\n",
        "    ax2.set_ylim(y_min, y_max)\n",
        "\n",
        "    # Add title\n",
        "    plt.title(f'{region}: Dynamics of Unemployment and Crime Indices',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # Add legend\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=11)\n",
        "\n",
        "    # Add number of countries for each point\n",
        "    for i, year in enumerate(years_list):\n",
        "        count = yearly_stats[year]['count']\n",
        "        ax1.annotate(f'n={count}', (year, unemp_means[i]),\n",
        "                    textcoords=\"offset points\", xytext=(0,15),\n",
        "                    ha='center', fontsize=9, alpha=0.8, color=color1,\n",
        "                    weight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate overall correlation for this region\n",
        "    all_unemp = region_data['UnemploymentIndex'].values.tolist()\n",
        "    all_crime = region_data['CrimeIndex'].values.tolist()\n",
        "    unemp_cleaned = replace_outliers_with_median(all_unemp)\n",
        "    crime_cleaned = replace_outliers_with_median(all_crime)\n",
        "    overall_corr, overall_pvalue = calculate_spearman_correlation(unemp_cleaned, crime_cleaned)\n",
        "\n",
        "    # Output brief report\n",
        "    print(f\"\\n{region} - Brief Report:\")\n",
        "    if len(years_list) >= 2:\n",
        "        unemp_trend = \"increase\" if unemp_means[-1] > unemp_means[0] else \"decrease\"\n",
        "        crime_trend = \"increase\" if crime_means[-1] > crime_means[0] else \"decrease\"\n",
        "        print(f\"  Unemployment trend: {unemp_trend} ({unemp_means[0]:.2f} -> {unemp_means[-1]:.2f})\")\n",
        "        print(f\"  Crime trend: {crime_trend} ({crime_means[0]:.2f} -> {crime_means[-1]:.2f})\")\n",
        "        print(f\"  Overall correlation: {overall_corr:.3f} ({'positive' if overall_corr > 0 else 'negative'})\")\n",
        "\n",
        "    print(f\"  Period: {min(years_list)}-{max(years_list)} ({len(years_list)} years)\")\n",
        "    print(f\"  Average countries per year: {calculate_mean([yearly_stats[y]['count'] for y in years_list]):.1f}\")\n",
        "\n",
        "# 12. SUMMARY TABLES\n",
        "print(\"\\n=== SUMMARY RESULTS WITH P-VALUE ===\")\n",
        "\n",
        "# Collect all data by regions for overall analysis\n",
        "overall_stats = {}\n",
        "\n",
        "for region in regions:\n",
        "    if region not in correlation_results or len(correlation_results[region]) == 0:\n",
        "        continue\n",
        "\n",
        "    # Collect all data for this region across all years\n",
        "    region_data = df_clean[df_clean['Region'] == region]\n",
        "    all_unemp = region_data['UnemploymentIndex'].values.tolist()\n",
        "    all_crime = region_data['CrimeIndex'].values.tolist()\n",
        "\n",
        "    # Handle anomalies\n",
        "    unemp_cleaned = replace_outliers_with_median(all_unemp)\n",
        "    crime_cleaned = replace_outliers_with_median(all_crime)\n",
        "\n",
        "    # Calculate overall correlation and p-value\n",
        "    overall_corr, overall_pvalue = calculate_spearman_correlation(unemp_cleaned, crime_cleaned)\n",
        "\n",
        "    # Collect yearly correlations\n",
        "    yearly_correlations = []\n",
        "    yearly_pvalues = []\n",
        "    for year_data in correlation_results[region].values():\n",
        "        yearly_correlations.append(year_data['correlation'])\n",
        "        if year_data['p_value'] is not None:\n",
        "            yearly_pvalues.append(year_data['p_value'])\n",
        "\n",
        "    avg_yearly_correlation = calculate_mean(yearly_correlations)\n",
        "    avg_yearly_pvalue = calculate_mean(yearly_pvalues) if yearly_pvalues else None\n",
        "\n",
        "    overall_stats[region] = {\n",
        "        'overall_correlation': overall_corr,\n",
        "        'overall_pvalue': overall_pvalue,\n",
        "        'avg_yearly_correlation': avg_yearly_correlation,\n",
        "        'avg_yearly_pvalue': avg_yearly_pvalue,\n",
        "        'years_count': len(yearly_correlations),\n",
        "        'countries_count': len(region_data['Country Code'].unique()),\n",
        "        'total_observations': len(region_data)\n",
        "    }\n",
        "\n",
        "# Detailed table with p-values\n",
        "print(\"\\nDetailed Table by Regions with p-value:\")\n",
        "print(\"Region\\t\\t\\tOverall.corr\\tp-value\\t\\tAvg.yearly\\tp-val(yearly)\\tYears\\tCountries\\tObs.\")\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for region in regions:\n",
        "    if region not in overall_stats:\n",
        "        continue\n",
        "\n",
        "    stats = overall_stats[region]\n",
        "\n",
        "    # Format p-values with stars for significance\n",
        "    def format_pvalue(pval):\n",
        "        if pval is None:\n",
        "            return \"N/A\\t\"\n",
        "        significance = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"* \" if pval < 0.05 else \"\"\n",
        "        return f\"{pval:.4f}{significance}\\t\"\n",
        "\n",
        "    overall_pval_str = format_pvalue(stats['overall_pvalue'])\n",
        "    yearly_pval_str = format_pvalue(stats['avg_yearly_pvalue'])\n",
        "\n",
        "    print(f\"{region:<20}\\t{stats['overall_correlation']:.3f}\\t\\t{overall_pval_str}\"\n",
        "          f\"{stats['avg_yearly_correlation']:.3f}\\t\\t{yearly_pval_str}\"\n",
        "          f\"{stats['years_count']}\\t{stats['countries_count']}\\t{stats['total_observations']}\")\n",
        "\n",
        "# Overall significance assessment\n",
        "print(f\"\\n=== OVERALL SIGNIFICANCE ASSESSMENT ===\")\n",
        "significant_regions = []\n",
        "for region, stats in overall_stats.items():\n",
        "    if stats['overall_pvalue'] is not None and stats['overall_pvalue'] < 0.05:\n",
        "        significant_regions.append(region)\n",
        "\n",
        "print(f\"Regions with statistically significant correlation (p < 0.05): {len(significant_regions)}\")\n",
        "for region in significant_regions:\n",
        "    stats = overall_stats[region]\n",
        "    strength = \"strong\" if abs(stats['overall_correlation']) > 0.7 else \\\n",
        "               \"moderate\" if abs(stats['overall_correlation']) > 0.3 else \"weak\"\n",
        "    direction = \"positive\" if stats['overall_correlation'] > 0 else \"negative\"\n",
        "    print(f\"   {region}: {strength} {direction} correlation \"\n",
        "          f\"(r={stats['overall_correlation']:.3f}, p={stats['overall_pvalue']:.4f})\")\n",
        "\n",
        "# Legend for p-values\n",
        "print(f\"\\nSignificance Legend:\")\n",
        "print(f\"   *** p < 0.001 (highly significant)\")\n",
        "print(f\"   **  p < 0.01  (very significant)\")\n",
        "print(f\"   *   p < 0.05  (significant)\")\n",
        "print(f\"   no stars: p ≥ 0.05 (not significant)\")\n",
        "\n",
        "# Overall statistics\n",
        "total_observations = len(df_clean)\n",
        "total_countries = len(df_clean['Country Code'].unique())\n",
        "year_range = f\"{min(df_clean['Year'])}-{max(df_clean['Year'])}\"\n",
        "\n",
        "print(f\"\\n=== OVERALL INFORMATION ===\")\n",
        "print(f\"Total number of observations: {total_observations}\")\n",
        "print(f\"Total number of countries: {total_countries}\")\n",
        "print(f\"Analysis period: {year_range}\")\n",
        "print(f\"Number of regions: {len(regions)}\")\n",
        "print(f\"Regions with significant correlation: {len(significant_regions)}\")"
      ]
    }
  ]
}